{"accuracy": 0.48710217755443885, "num_prompts": 2985, "model": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__d9f438c023f34dc88901546eb82e57de__SWAPS_2101", "ref_model": null, "tokenizer": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__d9f438c023f34dc88901546eb82e57de__SWAPS_2101", "chat_template": null, "extra_results": {"alpacaeval-easy": 0.6, "alpacaeval-hard": 0.29473684210526313, "alpacaeval-length": 0.5368421052631579, "donotanswer": 0.6544117647058824, "hep-cpp": 0.5121951219512195, "hep-go": 0.5365853658536586, "hep-java": 0.5304878048780488, "hep-js": 0.4817073170731707, "hep-python": 0.573170731707317, "hep-rust": 0.5365853658536586, "llmbar-adver-GPTInst": 0.41304347826086957, "llmbar-adver-GPTOut": 0.5106382978723404, "llmbar-adver-manual": 0.5434782608695652, "llmbar-adver-neighbor": 0.5, "llmbar-natural": 0.48, "math-prm": 0.2595078299776286, "mt-bench-easy": 0.6428571428571429, "mt-bench-hard": 0.43243243243243246, "mt-bench-med": 0.45, "refusals-dangerous": 0.89, "refusals-offensive": 0.77, "xstest-should-refuse": 0.564935064935065, "xstest-should-respond": 0.332}}