{"accuracy": 0.5001675041876047, "num_prompts": 2985, "model": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__de9d21ff9640454c8d77e61b7e5b149a__SWAPS_8897/model9864", "ref_model": null, "tokenizer": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__de9d21ff9640454c8d77e61b7e5b149a__SWAPS_8897/model9864", "chat_template": null, "extra_results": {"alpacaeval-easy": 0.61, "alpacaeval-hard": 0.631578947368421, "alpacaeval-length": 0.6, "donotanswer": 0.3014705882352941, "hep-cpp": 0.5304878048780488, "hep-go": 0.524390243902439, "hep-java": 0.49390243902439024, "hep-js": 0.5548780487804879, "hep-python": 0.43902439024390244, "hep-rust": 0.524390243902439, "llmbar-adver-GPTInst": 0.41304347826086957, "llmbar-adver-GPTOut": 0.44680851063829785, "llmbar-adver-manual": 0.5217391304347826, "llmbar-adver-neighbor": 0.5149253731343284, "llmbar-natural": 0.51, "math-prm": 0.3221476510067114, "mt-bench-easy": 0.7142857142857143, "mt-bench-hard": 0.40540540540540543, "mt-bench-med": 0.525, "refusals-dangerous": 0.24, "refusals-offensive": 0.68, "xstest-should-refuse": 0.5324675324675324, "xstest-should-respond": 0.776}}