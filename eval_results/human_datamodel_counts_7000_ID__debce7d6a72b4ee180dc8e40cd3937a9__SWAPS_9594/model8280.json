{"accuracy": 0.4368509212730318, "num_prompts": 2985, "model": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__debce7d6a72b4ee180dc8e40cd3937a9__SWAPS_9594/model8280", "ref_model": null, "tokenizer": "/root/autodl-tmp/data/result/merged_model/human_datamodel_counts_7000_ID__debce7d6a72b4ee180dc8e40cd3937a9__SWAPS_9594/model8280", "chat_template": null, "extra_results": {"alpacaeval-easy": 0.16, "alpacaeval-hard": 0.22105263157894736, "alpacaeval-length": 0.21052631578947367, "donotanswer": 0.7058823529411765, "hep-cpp": 0.42073170731707316, "hep-go": 0.5182926829268293, "hep-java": 0.42073170731707316, "hep-js": 0.3719512195121951, "hep-python": 0.43902439024390244, "hep-rust": 0.4695121951219512, "llmbar-adver-GPTInst": 0.8043478260869565, "llmbar-adver-GPTOut": 0.6382978723404256, "llmbar-adver-manual": 0.5652173913043478, "llmbar-adver-neighbor": 0.5298507462686567, "llmbar-natural": 0.42, "math-prm": 0.4228187919463087, "mt-bench-easy": 0.2857142857142857, "mt-bench-hard": 0.6216216216216216, "mt-bench-med": 0.45, "refusals-dangerous": 0.57, "refusals-offensive": 0.58, "xstest-should-refuse": 0.5194805194805194, "xstest-should-respond": 0.168}}